# ğŸš€ LearningAI - My AI/ML Learning Journey

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.12+-3776ab?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-Latest-ee4c2c?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)
[![Status](https://img.shields.io/badge/Status-In%20Progress-yellow?style=for-the-badge)](#)

A comprehensive repository documenting my journey through AI and Machine Learning fundamentals, from basic neural networks to physics-based models.

[âœ¨ Features](#features) â€¢ [ğŸ“ Directory Structure](#-directory-structure) â€¢ [ğŸ“ Learning Path](#-learning-path) â€¢ [ğŸ”— Resources](#-resources)

</div>

---

## ğŸ¯ Overview

This repository serves as my personal learning hub for AI/ML concepts and implementations. It contains:
- **Foundational concepts** from tutorials and textbooks
- **Hands-on implementations** using PyTorch
- **Physics-based models** and simulations
- **Progressive complexity** from basics to advanced topics

Each project includes detailed notebooks with explanations, code, and results.

---

## âœ¨ Features

- ğŸ“š **Tutorial-Based Learning** - Follow along with industry-standard courses
- ğŸ”¬ **Practical Implementations** - Real code, not just theory
- ğŸ“Š **Jupyter Notebooks** - Interactive learning with visualizations
- ğŸ“ **Well-Documented** - Comments and explanations throughout
- ğŸ§  **Progressive Difficulty** - Start simple, build complexity
- ğŸš€ **Production-Ready Code** - Clean, modular implementations

---

## ğŸ“ Directory Structure

```
LearningAI/
â”œâ”€â”€ Basics/
â”‚   â”œâ”€â”€ biagram_model/
â”‚   â”‚   â”œâ”€â”€ makemore.ipynb              # ğŸ“Œ [Andrej Karpathy - Makemore Tutorial]
â”‚   â”‚   â””â”€â”€ names.txt                   # Dataset: 32K English baby names
â”‚   â””â”€â”€ py_torch_basics.py              # PyTorch fundamentals
â”‚
â”œâ”€â”€ PhysicsBased/
â”‚   â””â”€â”€ basics.py                       # Physics-based model implementations
â”‚
â””â”€â”€ README.md                           # You are here! ğŸ‘ˆ
```

### ğŸ“š Basics Directory
The `Basics` folder contains foundational concepts and implementations:

#### ğŸ¬ Biagram Model (Makemore)
- **Tutorial**: [Makemore - Andrej Karpathy](https://www.youtube.com/watch?v=PaCmpygFfXo)
- **Content**: Building a character-level language model to generate baby names
- **Key Concepts**:
  - Bigram probability distributions
  - Character encoding/decoding
  - PyTorch tensor operations
  - Probability sampling with generators
- **Dataset**: 32,033 English baby names
- **Output**: Generated synthetic names based on learned patterns

#### ğŸ”§ PyTorch Basics
- Fundamental PyTorch operations
- Tensor manipulations
- Basic neural network concepts

### ğŸ§¬ PhysicsBased Directory
Advanced implementations incorporating physics principles:
- Physics-informed neural networks
- Conservation laws
- Differential equations
- Simulation-based learning

---

## ğŸ“ Learning Path

### Phase 1: Foundations (Current) âœ…
- [x] Character-level language models
- [x] Probability distributions
- [x] PyTorch basics
- [ ] Multi-layer perceptrons

### Phase 2: Intermediate (Upcoming)
- [ ] Recurrent Neural Networks (RNNs)
- [ ] Attention mechanisms
- [ ] Transformer architectures
- [ ] Fine-tuning pre-trained models

### Phase 3: Advanced (Future)
- [ ] Physics-informed neural networks (PINNs)
- [ ] Graph neural networks
- [ ] Reinforcement learning
- [ ] Diffusion models

---

## ğŸš€ Quick Start

### Prerequisites
```bash
python >= 3.12
uv (Fast Python package installer)
```

### Installation
```bash
# Clone the repository
git clone https://github.com/yourusername/LearningAI.git
cd LearningAI

# Create and sync virtual environment with uv
uv sync

# Activate the virtual environment
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

### Running the Notebooks
```bash
# Start Jupyter with uv
uv run jupyter notebook

# Open and run:
# - Basics/biagram_model/makemore.ipynb
# - Other notebooks as you explore
```

### Running Python Scripts
```bash
# Run PyTorch basics
uv run python Basics/py_torch_basics.py

# Run physics-based models
uv run python PhysicsBased/basics.py
```

### Installing Additional Dependencies
```bash
# Add new packages to the project
uv pip install package_name

# Or use uv add for managed dependencies
uv add package_name
```

---

## ğŸ“Š Project Highlights

### Makemore - Character-Level Language Model
This project implements a simple but elegant language model:

```
ğŸ“ˆ Model Architecture:
â”œâ”€â”€ Input Layer: Character embeddings
â”œâ”€â”€ Bigram Statistics: Probability distributions
â””â”€â”€ Output Layer: Next character prediction

ğŸ“Š Results:
â”œâ”€â”€ Vocabulary Size: 27 characters (a-z + special token)
â”œâ”€â”€ Training Data: 32,033 names
â””â”€â”€ Sample Output: Generated realistic names from learned patterns
```

**Key Learnings:**
- Building probability distributions from data
- Character encoding strategies
- Sampling from distributions
- Data visualization with matplotlib

---

## ğŸ”— Resources

### Recommended Tutorials & Courses
- **[Makemore Series](https://www.youtube.com/watch?v=PaCmpygFfXo)** - Andrej Karpathy's character-level language models
- **[Neural Networks: Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrZiCoo0dEwnNB1zrVzay6fHMzxqn)** - Complete ML foundation course
- **[PyTorch Official Tutorials](https://pytorch.org/tutorials/)** - Learn PyTorch from the source

### Books
- "Deep Learning" by Goodfellow, Bengio, and Courville
- "Neural Networks from Scratch" by Trask
- "Physics-Informed Machine Learning" - Recent research papers

### Tools & Libraries
- ğŸ”¥ **PyTorch** - Deep learning framework
- ğŸ““ **Jupyter Notebooks** - Interactive computing
- ğŸ“Š **Matplotlib** - Data visualization
- ğŸ”¢ **NumPy** - Numerical computing

---

## ğŸ’¡ Key Concepts Covered

| Concept | Status | Location |
|---------|--------|----------|
| Character Encoding | âœ… Complete | `Basics/biagram_model/` |
| Probability Distributions | âœ… Complete | `Basics/biagram_model/` |
| PyTorch Tensors | âœ… Complete | `Basics/py_torch_basics.py` |
| Physics-Based Models | ğŸ”„ In Progress | `PhysicsBased/` |
| RNNs | â³ Planned | TBD |
| Transformers | â³ Planned | TBD |

---

## ğŸ¤ Contributing

This is a personal learning repository, but I welcome suggestions! Feel free to:
- Report issues or corrections
- Suggest improvements
- Share learning resources
- Discuss concepts

---

## ğŸ“ Notes & Documentation

Each file includes:
- **Comments**: Inline explanations of complex logic
- **Docstrings**: Function and module documentation
- **Markdown cells** (in notebooks): Concept explanations
- **Output examples**: Expected results and visualizations

---

## ğŸ¯ Goals & Objectives

**Short Term (Next 3 months):**
- âœ… Master character-level language models
- â³ Implement RNNs from scratch
- â³ Build a simple transformer

**Medium Term (Next 6 months):**
- â³ Implement attention mechanisms
- â³ Explore transfer learning
- â³ Create physics-informed models

**Long Term (Next Year):**
- â³ Build advanced neural architectures
- â³ Contribute to open-source ML projects
- â³ Create production-ready models

---

## ğŸ”® Future Additions

- [ ] Recurrent Neural Networks (RNNs)
- [ ] Long Short-Term Memory (LSTM) networks
- [ ] Gated Recurrent Units (GRUs)
- [ ] Attention Mechanisms
- [ ] Transformer from scratch
- [ ] Vision Transformers (ViT)
- [ ] Physics-Informed Neural Networks (PINNs)
- [ ] Graph Neural Networks (GNNs)
- [ ] Reinforcement Learning fundamentals
- [ ] Generative models (VAE, GAN, Diffusion)

---

## ğŸ“š Notebook Descriptions

### `Basics/biagram_model/makemore.ipynb`
**Status:** âœ… Complete  
**Time to Complete:** ~2 hours  
**Difficulty:** Beginner  

A comprehensive walkthrough of building a character-level language model using bigram statistics. Starting from raw data loading to generating synthetic names, this notebook covers all the fundamentals needed to understand how language models work at the most basic level.

**Topics Covered:**
- Data loading and preprocessing
- Bigram extraction and counting
- Probability matrix construction
- Visualization of statistics
- Sampling from distributions
- Name generation

---

## ğŸ› ï¸ Tech Stack

```
Backend:
â”œâ”€â”€ Python 3.12+
â”œâ”€â”€ PyTorch 2.0+
â”œâ”€â”€ NumPy
â””â”€â”€ Matplotlib

Development:
â”œâ”€â”€ Jupyter Notebook
â”œâ”€â”€ Git & GitHub
â””â”€â”€ VS Code / Cursor IDE
```

---

## ğŸ“ Get In Touch

- **GitHub**: [Your GitHub Profile]
- **Twitter**: [@YourHandle]
- **LinkedIn**: [Your LinkedIn]

---

## â­ If This Repo Helped You!

If you found this repository useful for your own learning journey, please consider:
- â­ Starring this repository
- ğŸ”„ Sharing it with others
- ğŸ’¬ Leaving feedback and suggestions

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

<div align="center">

### ğŸš€ Happy Learning! Keep Building, Keep Improving!

**Last Updated:** December 22, 2025  
**Last Modified:** 2 weeks ago

[â¬† Back to Top](#-learningai---my-aiml-learning-journey)

</div>

